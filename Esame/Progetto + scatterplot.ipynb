{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo tutte le librerie che ci interessano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.layers import Input, Conv2D, Dense, MaxPooling2D , Flatten\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from collections import deque\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample \n",
    "import chart_studio.plotly as py\n",
    "from plotly.graph_objs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo l'ambiente SpaceInvaders-vo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo la nostra rete neurale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, input_shape, output_shape, discount=0.99, update_target_every=10, memory_size=2000):\n",
    "        self.input_shape=input_shape\n",
    "        self.output_shape=output_shape\n",
    "        self.discount=discount\n",
    "        self.update_target_every=update_target_every\n",
    "        self.policy_net=self.create_model()\n",
    "        self.memory=deque(maxlen=memory_size)\n",
    "        self.target_counter=0 \n",
    "    \n",
    "    def create_model(self):\n",
    "        model=Sequential()\n",
    "        model.add(Conv2D(input_shape=self.input_shape, filters=16, kernel_size=(8,8), strides=(4,4), padding=\"valid\", \n",
    "                        activation=\"relu\", use_bias=True,))\n",
    "        model.add(Conv2D(filters=16, kernel_size=(4,4), strides=(2,2), padding=\"valid\", \n",
    "                       activation=\"relu\", use_bias=True,))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation=\"relu\"))\n",
    "        model.add(Dense(self.output_shape)) \n",
    "        adm=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(loss=\"mse\", optimizer=adm, metrics=[\"accuracy\"] )\n",
    "        return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo tre metodi per preprocessare la nostra immagine croppandola e portandola ad una scala di grigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_greyscale(img):\n",
    "        return np.mean(img , axis=2).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(img):\n",
    "        return img[::2 , ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    return img[:100 ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "        return crop(to_greyscale(downsample(img)))/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = env.reset()\n",
    "plt.imshow(preprocess(img))\n",
    "print(preprocess(img).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(to_greyscale(downsample(img)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience_Replay:\n",
    "    def __init__(self , memory = [] , maxsize = 10000):\n",
    "            self.iteration = 0\n",
    "            self.memory = memory\n",
    "            self.maxsize = maxsize\n",
    "            self.experience_gain = []\n",
    "            self.rmse = []\n",
    "            \n",
    "    def getSample(self, size_sample=32):\n",
    "        choises =np.random.randint(len(experience.memory), size=size_sample )\n",
    "        arr= np.array(experience.memory)\n",
    "        return arr[choises , :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(env  , model  , experience):\n",
    "    env.reset()\n",
    "    epsilon =  (0.995)**(experience.iteration)\n",
    "    o,r,d,i = env.step(0)\n",
    "    env.render()\n",
    "    processed_state = preprocess(o)\n",
    "    processed_state = processed_state.reshape((1,100,80,1))\n",
    "    experience.memory = []\n",
    "    exp_gain = 0\n",
    "    for i in range(experience.maxsize):\n",
    "        if  random.random() < epsilon:\n",
    "            ac = env.action_space.sample()\n",
    "        else:\n",
    "            ac = np.argmax(model.policy_net.predict(processed_state))\n",
    "        next_state,reward,done,info = env.step(ac)\n",
    "        exp_gain= exp_gain+ reward\n",
    "        if done:\n",
    "            env.reset()\n",
    "        env.render()\n",
    "        next_processed_state = preprocess(next_state)\n",
    "        next_processed_state = next_processed_state.reshape((1,100,80,1))\n",
    "        experience.memory.append((processed_state,ac  , next_processed_state , reward, done))\n",
    "        processed_state=next_processed_state\n",
    "    experience.iteration=experience.iteration+1 \n",
    "    experience.experience_gain.append(exp_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_experience_replay(experience_replay):\n",
    "    #experience_replay= experience_replay[44:]\n",
    "    length = len(experience_replay)-1\n",
    "    new_experience_replay = []\n",
    "    i=0\n",
    "    while i < length:\n",
    "        x1=(experience_replay[i])\n",
    "        x2=(experience_replay[i+1])\n",
    "        x3 = [np.maximum(x1[0], x2[0]) , x2[1] ,np.maximum(x1[2], x2[2]) , x1[3]+x2[3],x2[4]]\n",
    "        new_experience_replay.append(x3)\n",
    "        i=i+4\n",
    "    return new_experience_replay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train ( experience , model,gamma = 0.99 , train_iteration=20):\n",
    "    experience.memory = preprocess_experience_replay(experience.memory)\n",
    "    for i in range(train_iteration):\n",
    "        batch = experience.getSample()\n",
    "        for state , action , next_state , reward ,done in batch:\n",
    "            target =model.policy_net.predict(state)[0]\n",
    "            if not done:\n",
    "                target[action]= reward + gamma* np.max(model.policy_net.predict(next_state)[0])\n",
    "            else:\n",
    "                target[action]= 0\n",
    "            target =target.reshape(1,6)\n",
    "            model.policy_net.fit(state , target , verbose=0)\n",
    "            prediction = model.policy_net.predict(state)\n",
    "            experience.rmse.append(np.sqrt(np.mean((prediction-np.array(target))**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = Experience_Replay(maxsize=3000)\n",
    "dqn = DQN((100,80,1),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.policy_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    iteration(env,dqn,experience)\n",
    "    train(experience , dqn)\n",
    "    env.close()\n",
    "    print(f\"iterazione: {experience.iteration}\\t gain:{experience.experience_gain[-1]}\\t esplorazione: {(0.995)**(experience.iteration)}\\t batch :{len(experience.memory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_network(env , model):\n",
    "    env.reset()\n",
    "    o,r,d,i = env.step(0)\n",
    "    env.render()\n",
    "    processed_state = preprocess(o)\n",
    "    processed_state = processed_state.reshape((1,100,80,1))\n",
    "    for i in range(10000):\n",
    "        ac = np.argmax(model.policy_net.predict(processed_state))\n",
    "        next_state,reward,done,info = env.step(ac)\n",
    "        if done:\n",
    "            break\n",
    "        env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "play_network(env , dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salviamo la rete\n",
    "dqn.policy_net.save(\"modello_prova.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scatterplot (title, y, x_title, y_title, x_upperBound, y_upperBound) : \n",
    "    '''\n",
    "        Print a scatter plot with:\n",
    "            x-axis: sequential integer \n",
    "            y-axis: list y's values\n",
    "        \n",
    "        Args:\n",
    "            title (str): plot's title\n",
    "            y (list): n-value of n-iterations \n",
    "            x_title (str): x-axis's title\n",
    "            y_title (str): y-axis's title\n",
    "            x_upperBound (int): x-axis's upper bound\n",
    "            y_upperBound (int): y-axis's upper bound \n",
    "        \n",
    "        Code to install modules required:\n",
    "            pip install plotly\n",
    "            pip install \"ipywidgets>=7.2\"\n",
    "            pip install chart-studio\n",
    "        \n",
    "        Code to import modules required:\n",
    "            import chart_studio.plotly as py\n",
    "            from plotly.graph_objs import *\n",
    "    '''\n",
    "      \n",
    "    py.sign_in('mickPar', 'vc7gPeON5gTXA6gmfrO7')\n",
    "    x_value = []\n",
    "    y_value = []\n",
    "    for index, rmse in enumerate(y) :\n",
    "        x_value.append(index)\n",
    "        y_value.append(rmse)\n",
    "\n",
    "    trace1 = {\n",
    "      \"uid\": \"5eacaf\", \n",
    "      \"name\": \"RMSE (Root Mean Square Error)\", \n",
    "      \"type\": \"scatter\", \n",
    "      \"x\": x_value, \n",
    "      \"y\": y_value\n",
    "    }\n",
    "    \n",
    "    data = Data([trace1])\n",
    "    layout = {\n",
    "          \"title\": title, \n",
    "      \"width\": 1050, \n",
    "      \"xaxis\": {\n",
    "        \"type\": \"linear\", \n",
    "        \"range\": [0, x_upperBound], \n",
    "        \"title\": x_title, \n",
    "        \"autorange\": False\n",
    "      }, \n",
    "      \"yaxis\": {\n",
    "        \"type\": \"linear\", \n",
    "        \"range\": [0, y_upperBound], \n",
    "        \"title\": y_title, \n",
    "        \"autorange\": False\n",
    "      }, \n",
    "      \"height\": 793, \n",
    "      \"autosize\": True, \n",
    "      \"annotations\": [\n",
    "        {\n",
    "          \"x\": x_upperBound - x_upperBound/3, \n",
    "          \"y\": y_upperBound, \n",
    "          \"font\": {\"size\": 16}, \n",
    "          \"text\": \"α (Learning rate) = 0.001, epsilon: 0.995**(#episodes)\", \n",
    "          \"showarrow\": False\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    fig = Figure(data=data, layout=layout)\n",
    "    # per aprire il grafico nel browser (per condividere, creare dashboard...)\n",
    "    # plot_url = py.plot(fig)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_upperBound = len(experience.rmse)\n",
    "y_upperBound = max(experience.rmse)\n",
    "\n",
    "custom_scatterplot(\"RMSE vs #ITERATIONS\", experience.rmse, '# episodes', 'Root mean square error', x_upperBound, y_upperBound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = []\n",
    "nrEpisodes = 500\n",
    "for i in range (nrEpisodes): \n",
    "    epsilon.append(0.995**i)\n",
    "custom_scatterplot('ϵ decay', epsilon, '# episodes', 'epsilon', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience.experience_gain\n",
    "x_upperBound = len(experience.experience_gain)\n",
    "y_upperBound = max(experience.experience_gain)\n",
    "\n",
    "custom_scatterplot('Return vs Experience', experience.experience_gain, 'n-experience', 'Return',  x_upperBound, y_upperBound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
