{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo tutte le librerie che ci interessano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.layers import Input, Conv2D, Dense, MaxPooling2D , Flatten\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from collections import deque\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo l'ambiente SpaceInvaders-vo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo la nostra rete neurale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, input_shape, output_shape, discount=0.99, update_target_every=10, memory_size=2000):\n",
    "        self.input_shape=input_shape\n",
    "        self.output_shape=output_shape\n",
    "        self.discount=discount\n",
    "        self.update_target_every=update_target_every\n",
    "        self.policy_net=self.create_model()\n",
    "        self.memory=deque(maxlen=memory_size)\n",
    "        self.target_counter=0 \n",
    "    \n",
    "    def create_model(self):\n",
    "        model=Sequential()\n",
    "        model.add(Conv2D(input_shape=self.input_shape, filters=16, kernel_size=(8,8), strides=(4,4), padding=\"valid\", \n",
    "                        activation=\"relu\", use_bias=True,))\n",
    "        model.add(Conv2D(filters=16, kernel_size=(4,4), strides=(2,2), padding=\"valid\", \n",
    "                       activation=\"relu\", use_bias=True,))\n",
    "        #model.add(Conv2D(filters=32, kernel_size=(2,2), padding=\"valid\", \n",
    "        #                activation=\"relu\", use_bias=True,))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation=\"relu\"))\n",
    "        model.add(Dense(self.output_shape)) # era softmax\n",
    "        adm=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(loss=\"mse\", optimizer=adm, metrics=[\"accuracy\"] )\n",
    "        return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo tre metodi per preprocessare la nostra immagine croppandola e portandola ad una scala di grigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_greyscale(img):\n",
    "        return np.mean(img , axis=2).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(img):\n",
    "        return img[::2 , ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    return img[10:100 ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.reset()\n",
    "plt.imshow(preprocess(img))\n",
    "#print(preprocess(img).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(to_greyscale(downsample(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "        return crop(to_greyscale(downsample(img)))/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo un metodo per trasformare ogni ricompensa in (-1, 0, +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_reward(reward):\n",
    "    return np.sign(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self , memory = [] , maxsize = 10000):\n",
    "            self.iteration = 0\n",
    "            self.memory = memory\n",
    "            self.maxsize = maxsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(env  , model  , mem):\n",
    "    env.reset()\n",
    "    epsilon =  (0.995)**(mem.iteration)\n",
    "    o,r,d,i = env.step(0)\n",
    "    env.render()\n",
    "    processed_state = preprocess(o)\n",
    "    processed_state = processed_state.reshape((1,90,80,1))\n",
    "    mem.memory = []\n",
    "    for i in range(mem.maxsize):\n",
    "        if  random.random() < epsilon:\n",
    "            ac = env.action_space.sample()\n",
    "        else:\n",
    "            ac = np.argmax(model.policy_net.predict(processed_state))\n",
    "        next_state,reward,done,info = env.step(ac)\n",
    "        if done:\n",
    "            next_processed_state = preprocess(next_state)\n",
    "            next_processed_state = next_processed_state.reshape((1,90,80,1))\n",
    "            mem.memory.append((processed_state,ac  , next_processed_state , reward, done ))\n",
    "            break\n",
    "        env.render()\n",
    "        next_processed_state = preprocess(next_state)\n",
    "        next_processed_state = next_processed_state.reshape((1,90,80,1))\n",
    "        mem.memory.append((processed_state,ac  , next_processed_state , reward, done ))\n",
    "        processed_state=next_processed_state\n",
    "    mem.iteration=mem.iteration+1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory()\n",
    "dqn = DQN((90,80,1),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.policy_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration(env , dqn , mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mem.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration(env , dqn , mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mem.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem.iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.policy_net.predict(mem.memory[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, _  in enumerate(mem.memory):\n",
    "    print(index , dqn.policy_net.predict(mem.memory[index][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, _  in enumerate(mem.memory):\n",
    "    print(index ,mem.memory[index][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train ( mem , model,gamma = 0.99 ):\n",
    "    for state , action , next_state , reward ,done in mem.memory:\n",
    "            #rewards = np.zeros(6)\n",
    "            #rewards[action]=reward\n",
    "            #q_values = np.zeros(6)\n",
    "            #q_values[best]=1\n",
    "            #target = rewards+ (  gamma * q_values)\n",
    "            #target = target.reshape(1,6)\n",
    "            target =model.policy_net.predict(state)[0]\n",
    "            if not done:\n",
    "                #best = np.argmax(model.policy_net.predict(next_state))\n",
    "                target[action]= reward + gamma* np.max(model.policy_net.predict(next_state)[0])\n",
    "            else:\n",
    "                target[action]= 0\n",
    "            target =target.reshape(1,6)\n",
    "            model.policy_net.fit(state , target , verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(mem , dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration(env,dqn,mem)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mem.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory()\n",
    "dqn = DQN((90,80,1),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    train(mem , dqn)\n",
    "    iteration(env,dqn,mem)\n",
    "    env.close()\n",
    "    print(f\"iterazione: {mem.iteration}\\t memoria: { len(mem.memory)}\\t esplorazione: {(0.995)**(mem.iteration+1)}\")\n",
    "    #print(\"memoria\" , len(mem.memory))\n",
    "    #print(\"esplorazione\" ,(0.995)**(mem.iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_network(env , model):\n",
    "    env.reset()\n",
    "    o,r,d,i = env.step(0)\n",
    "    env.render()\n",
    "    processed_state = preprocess(o)\n",
    "    processed_state = processed_state.reshape((1,90,80,1))\n",
    "    for i in range(10000):\n",
    "        ac = np.argmax(model.policy_net.predict(processed_state))\n",
    "        next_state,reward,done,info = env.step(ac)\n",
    "        if done:\n",
    "            break\n",
    "        env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "play_network(env , dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salviamo la rete\n",
    "dqn.policy_net.save(\"modello_prova.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mem.memory[540][0].reshape((90,80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
